{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction using Quadgram\n",
    "This program reads the corpus line by line.This reads the corpus one line at a time loads it into the memory\n",
    "### Time Complexity for word prediction : O(1)\n",
    "### Time Complexity for word prediction with rank 'r': O(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Import corpus</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Do preprocessing</u>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the punctuations and lowercase the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns: string\n",
    "#arg: string\n",
    "#remove punctuations and make the string lowercase\n",
    "def removePunctuations(sen):\n",
    "\n",
    "    #split the string into word tokens\n",
    "    temp_l = sen.split()\n",
    "    i = 0\n",
    "\n",
    "    #changes the word to lowercase and removes punctuations from it\n",
    "    for word in temp_l :\n",
    "        for l in word :\n",
    "            if l in string.punctuation:\n",
    "                word = word.replace(l,\" \")\n",
    "        temp_l[i] = word.lower()\n",
    "        i=i+1   \n",
    "\n",
    "    #spliting is being don here beacause in sentences line here---so after punctuation removal it should \n",
    "    #become \"here so\"   \n",
    "    content = \" \".join(temp_l)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and load the corpus data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns : void\n",
    "#arg: string,dict,dict,dict,dict\n",
    "#loads the corpus for the dataset and makes the frequency count of quadgram and trigram strings\n",
    "def loadCorpus(file_path,bi_dict,tri_dict,quad_dict,vocab_dict):\n",
    "\n",
    "    w1 = ''    #for storing the 3rd last word to be used for next token set\n",
    "    w2 = ''    #for storing the 2nd last word to be used for next token set\n",
    "    w3 = ''    #for storing the last word to be used for next token set\n",
    "    token = []\n",
    "    word_len = 0\n",
    "\n",
    "    #open the corpus file and read it line by line\n",
    "    with open(file_path,'r') as file:\n",
    "        for line in file:\n",
    "\n",
    "            #split the line into tokens\n",
    "            token = line.split()\n",
    "            i = 0\n",
    "\n",
    "            #for each word in the token list ,remove pucntuations and change to lowercase\n",
    "            for word in token :\n",
    "                for l in word :\n",
    "                    if l in string.punctuation:\n",
    "                        word = word.replace(l,\" \")\n",
    "                token[i] = word.lower()\n",
    "                i=i+1\n",
    "\n",
    "            #make the token list into a string    \n",
    "            content = \" \".join(token)\n",
    "            token = content.split()\n",
    "            word_len = word_len + len(token)  \n",
    "\n",
    "            if not token:\n",
    "                continue\n",
    "\n",
    "            #add the last word from previous line\n",
    "            if w3!= '':\n",
    "                token.insert(0,w3)\n",
    "\n",
    "            temp0 = list(ngrams(token,2))\n",
    "\n",
    "            #since we are reading line by line some combinations of word might get missed for pairing\n",
    "            #for trigram\n",
    "            #first add the previous words\n",
    "            if w2!= '':\n",
    "                token.insert(0,w2)\n",
    "\n",
    "            #tokens for trigrams\n",
    "            temp1 = list(ngrams(token,3))\n",
    "\n",
    "            #insert the 3rd last word from previous line for quadgram pairing\n",
    "            if w1!= '':\n",
    "                token.insert(0,w1)\n",
    "\n",
    "            #add new unique words to the vocaulary set if available\n",
    "            for word in token:\n",
    "                if word not in vocab_dict:\n",
    "                    vocab_dict[word] = 1\n",
    "                else:\n",
    "                    vocab_dict[word]+= 1\n",
    "              \n",
    "            #tokens for quadgrams\n",
    "            temp2 = list(ngrams(token,4))\n",
    "\n",
    "            #count the frequency of the bigram sentences\n",
    "            for t in temp0:\n",
    "                sen = ' '.join(t)\n",
    "                bi_dict[sen] += 1\n",
    "\n",
    "            #count the frequency of the trigram sentences\n",
    "            for t in temp1:\n",
    "                sen = ' '.join(t)\n",
    "                tri_dict[sen] += 1\n",
    "\n",
    "            #count the frequency of the quadgram sentences\n",
    "            for t in temp2:\n",
    "                sen = ' '.join(t)\n",
    "                quad_dict[sen] += 1\n",
    "\n",
    "\n",
    "            #then take out the last 3 words\n",
    "            n = len(token)\n",
    "\n",
    "            #store the last few words for the next sentence pairing\n",
    "            w1 = token[n -3]\n",
    "            w2 = token[n -2]\n",
    "            w3 = token[n -1]\n",
    "    return word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Hash Table for Probable words for Trigram sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns: void\n",
    "#arg: dict,dict,dict,dict,dict,int\n",
    "#creates dict for storing probable words with their probabilities for a trigram sentence\n",
    "def createProbableWordDict(bi_dict,tri_dict,quad_dict,prob_dict,vocab_dict,token_len):\n",
    "    for quad_sen in quad_dict:\n",
    "        prob = 0.0\n",
    "        quad_token = quad_sen.split()\n",
    "        tri_sen = ' '.join(quad_token[:3])\n",
    "        tri_count = tri_dict[tri_sen]\n",
    "\n",
    "        if tri_count != 0:\n",
    "            prob = interpolatedProbability(quad_token,token_len, vocab_dict, bi_dict, tri_dict, quad_dict,\n",
    "                            l1 = 0.25, l2 = 0.25, l3 = 0.25 , l4 = 0.25)\n",
    "\n",
    "        if tri_sen not in prob_dict:\n",
    "            prob_dict[tri_sen] = []\n",
    "            prob_dict[tri_sen].append([prob,quad_token[-1]])\n",
    "        else:\n",
    "            prob_dict[tri_sen].append([prob,quad_token[-1]])\n",
    "\n",
    "\n",
    "    prob = None\n",
    "    tri_count = None\n",
    "    quad_token = None\n",
    "    tri_sen = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the probable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns: void\n",
    "#arg: dict\n",
    "#for sorting the probable word acc. to their probabilities\n",
    "def sortProbWordDict(prob_dict):\n",
    "    for key in prob_dict:\n",
    "        if len(prob_dict[key])>1:\n",
    "            sorted(prob_dict[key],reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Driver function for doing the prediction</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns: string\n",
    "#arg: string,dict,int\n",
    "#does prediction for the the sentence\n",
    "def doPrediction(sen,prob_dict,rank = 1):\n",
    "    if sen in prob_dict:\n",
    "        if rank <= len(prob_dict[sen]):\n",
    "            return prob_dict[sen][rank-1][1]\n",
    "        else:\n",
    "            return prob_dict[sen][0][1]\n",
    "    else:\n",
    "        return \"Can't predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> For Computing Interpolated Probability</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns: float\n",
    "#arg: float,float,float,float,list,list,dict,dict,dict,dict\n",
    "#for calculating the interpolated probablity\n",
    "def interpolatedProbability(quad_token,token_len, vocab_dict, bi_dict, tri_dict, quad_dict,\n",
    "                            l1 = 0.25, l2 = 0.25, l3 = 0.25 , l4 = 0.25):\n",
    "    \n",
    "    sen = ' '.join(quad_token)\n",
    "    prob =(   \n",
    "              l1*(quad_dict[sen] / tri_dict[' '.join(quad_token[0:3])]) \n",
    "            + l2*(tri_dict[' '.join(quad_token[1:4])] / bi_dict[' '.join(quad_token[1:3])]) \n",
    "            + l3*(bi_dict[' '.join(quad_token[2:4])] / vocab_dict[quad_token[2]]) \n",
    "            + l4*(vocab_dict[quad_token[3]] / token_len)\n",
    "          )\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>For Taking input from the User</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns: string\n",
    "#arg: void\n",
    "#for taking input from user\n",
    "def takeInput():\n",
    "    cond = False\n",
    "    #take input\n",
    "    while(cond == False):\n",
    "        sen = input('Enter the string\\n')\n",
    "        sen = removePunctuations(sen)\n",
    "        temp = sen.split()\n",
    "        if len(temp) < 3:\n",
    "            print(\"Please enter atleast 3 words !\")\n",
    "        else:\n",
    "            cond = True\n",
    "            temp = temp[-3:]\n",
    "    sen = \" \".join(temp)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>main function</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-9-6275781483df>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-6275781483df>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    \"\"\"\"\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def main():\n",
    "\n",
    "    #variable declaration\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of three words\n",
    "    vocab_dict = defaultdict(int) #for storing the different words with their frequencies    \n",
    "    prob_dict = OrderedDict()   #for storing the probabilities of probable words for a sentence\n",
    "    bi_dict = defaultdict(int)\n",
    "\n",
    "    #load the corpus for the dataset\n",
    "    token_len = loadCorpus('corpusfile.txt',bi_dict,tri_dict,quad_dict,vocab_dict)\n",
    "    print(\"---Preprocessing Time for Corpus loading: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    start_time1 = time.time()\n",
    "\n",
    "    #creates a dictionary of probable words \n",
    "    createProbableWordDict(bi_dict,tri_dict,quad_dict,prob_dict,vocab_dict,token_len)\n",
    "    #sort the dictionary of probable words \n",
    "    sortProbWordDict(prob_dict)\n",
    "    # writeProbWords(prob_dict)\n",
    "    gc.collect()\n",
    "    print(\"---Preprocessing Time for Creating Probable Word Dict: %s seconds ---\" % (time.time() - start_time1))\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i><u>For Debugging Purpose Only</u></i>\n",
    "<i>Uncomment the above two cells and ignore running the cells below if not debugging</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing Time for Corpus loading: 22.767502307891846 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#variable declaration\n",
    "tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "quad_dict = defaultdict(int)           #for keeping count of sentences of three words\n",
    "vocab_dict = defaultdict(int) #for storing the different words with their frequencies    \n",
    "prob_dict = OrderedDict()   #for storing the probabilities of probable words for a sentence\n",
    "bi_dict = defaultdict(int)\n",
    "\n",
    "#load the corpus for the dataset\n",
    "token_len = loadCorpus('corpusfile.txt',bi_dict,tri_dict,quad_dict,vocab_dict)\n",
    "print(\"---Preprocessing Time for Corpus loading: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing Time for Creating Probable Word Dict: 17.206525087356567 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time1 = time.time()\n",
    "  \n",
    "#creates a dictionary of probable words \n",
    "createProbableWordDict(bi_dict,tri_dict,quad_dict,prob_dict,vocab_dict,token_len)\n",
    "#sort the dictionary of probable words \n",
    "sortProbWordDict(prob_dict)\n",
    "# writeProbWords(prob_dict)\n",
    "gc.collect()\n",
    "print(\"---Preprocessing Time for Creating Probable Word Dict: %s seconds ---\" % (time.time() - start_time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the string\n",
      "emma by jane\n",
      "Word Prediction: austen\n",
      "---Time for Prediction Operation: 0.0007853507995605469 seconds ---\n"
     ]
    }
   ],
   "source": [
    "sen = takeInput()\n",
    "start_time2 = time.time()\n",
    "prediction = doPrediction(sen,prob_dict)\n",
    "print(\"Word Prediction:\",prediction)\n",
    "print(\"---Time for Prediction Operation: %s seconds ---\" % (time.time() - start_time2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
